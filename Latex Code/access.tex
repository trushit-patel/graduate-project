\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage{tabularx}
\usepackage{balance}
\usepackage{multirow}%z
\usepackage{array} %Z
\usepackage{booktabs} %z
\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\history{Date of publication Dec 14, 2024}
\doi{}

\title{Veriforge:An Image Forgery Detection Model}
\author{\uppercase{Rishikesh Ravi}\authorrefmark{1},
\uppercase{Hamza Rangwala}\authorrefmark{2},
\uppercase{Ayaz Afzal}\authorrefmark{3} and \uppercase{Trushit Patel} \authorrefmark{4},
}

\address[1]{Wilfrid Laurier University, Waterloo ON Canada (email:rish8180@mylaurier.ca)}
\address[2]{Wilfrid Laurier University, Waterloo ON Canada (email:rang8720@mylaurier.ca)}
\address[3]{Wilfrid Laurier University, Waterloo ON Canada (email:afza9880@mylaurier.ca)}
\address[4]{Wilfrid Laurier University, Waterloo ON Canada (email:pate9410@mylaurier.ca)}


%\markboth
%{Rishikesh \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
%{Rishikesh \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}



\begin{abstract}
This paper proposes a new method of tampered image detection by combining the CNN with a backbone of ResNet50 with ELA. Our method is designed for digital forensics, which is a very important domain for the detection and assessment of image modifications in many other domains, such as cybersecurity, journalism, and criminal investigations. This procedure comprises image pre-processing to ascertain altered areas by comparing variations in compression levels to find compression artifacts using ELA. It serves as the foundation for reliable feature extraction: the ResNet50 model, pre-trained on ImageNet, has its basic layers frozen to preserve learned weights.
For customization with regard to binary classification, some custom layers are appended on top: global average pooling, batch normalization, dense layers with ReLU, and dropout regularization. The architecture here comes up with a generator-based approach so that large datasets can be handled with better memory consumption, enabling real-time data preprocessing during training of models. Our methodology is assured to optimize resource utilization and enhance generalization, which has been evidenced with extensive experiments. The obtained model offers high accuracy and robustness in detecting tampered images; thus, it presents a scalable solution for real-world forensic applications. This integration of ELA and CNN provides the backbone of fast and accurate analysis of digital evidence to help in better decision-making in sensitive situations.
\end{abstract}

\begin{keywords}
	Tampered Image Detection, Error Level Analysis (ELA), Convolutional Neural Network (CNN), ResNet50, Digital Forensics, Image Manipulation Detection, Cybersecurity, Journalism, Criminal Investigations, Feature Extraction, Binary Classification, Data Preprocessing, Dropout Regularization, Compression Artifacts, Real-time Analysis, Scalable Forensic Applications.
\end{keywords}

\titlepgskip=-21pt

\maketitle

\section{Introduction}
\label{sec:introduction}
\PARstart{I}{n} this digital era, where information is usually imparted through the means of digitization, a tampered image might start a wildfire of misinformation, controversies, and disasters in a lot of sectors. Over the last few decades, deep learning has grown to be one of the strongest and leading methods for the solution of challenging problems arising in numerous fields \cite{10035377}. More use of social media escalates the scale of it, which calls for this challenge to be taken seriously. The instances of tampered images being used for spreading fake news, malicing individuals or organizations, and manipulating public opinion have started becoming alarmingly common. Images are being manipulated through image processing tools or even completely fabricated with the help of AI, in order to distort the truth and mislead people or clinch wrongful convictions in a court of law \cite{8755865}. Our research focuses on developing a model called VeriForge, which is designed for the detection and exposure of image tampering using the power of Deep Learning and Error Level Analysis \cite{gupta2022detection}. This innovative approach is targeted at offering an effective solution to find discrepancies in digital images and building trust in visual content. However, accessibility to sophisticated image editing software has put the art of doctoring images within everybody's reach, irrespective of any kind of technical expertise. For that reason, it has become an increasingly demanding task to differentiate between original and tampered images by commoners, but also by professionals in vital fields like law and order or journalism. The available techniques of image tampering detection, though at one's fingertips, have numerous limitations. Most of the known methods are outdated, time-consuming, and cannot find a complex type of manipulation. Nowadays, there is a serious demand to create modern and effective methods which could cope with complications arising in image forgery techniques~\cite{kaur2019weak}.

An automatic computer-based system is required to be developed that can identify the originality of the input image \cite{9404748}. VeriForge fills this gap by using the synergy of traditional forensic methods and deep learning. ELA, for example, is a well-established method in forensics: it underlines inconsistencies between image compression levels, hence highlighting the image anomalies caused by editing \cite{singh2021image}. VeriForge couples ELA with deep learning to extract meaningful features from these discrepancies in a way that has allowed the attainment of high accuracy enabled by advanced neural network architecture. It allows VeriForge to find the tampered images with quite an intelligent approach.

For the training and verification of VeriForge, we utilize CASIA V2, a database of well-labeled images in their original and forged aspects. In this database, tampering is performed manually with several image editing tools; thus, it is one of the good sources for training a model to detect different types of tampering~\cite{7412439}. This is also ensured by the great variety of images in the dataset; thus, the model generalizes well to unseen data, avoiding overfitting and improving its application in real-world scenarios.

The VGG-16 architecture was adopted for the first version of VeriForge. Although a promising result could be achieved in a preliminary way, we obtained only mediocre performance, given that after some point the model converged, resulting in an inability to increase its accuracy. Then, in order to capture the intricate image features, residual learning with the much deeper convolutional neural network architecture, ResNet50~\cite{simonyan2015deepconvolutionalnetworkslargescale}, was chosen. This switch to ResNet50 allowed VeriForge to capture more complicated patterns of tampered images, thereby increasing its detection capability by a big margin.


The general objective of the work is to protect the integrity of digital content by offering a reliable, scalable, and efficient tool for image forgery detection. In this regard, VeriForge uses advanced techniques such as convolutional neural networks (CNNs) \cite{thepade2021image}, transfer learning \cite{han2020scene}, and state-of-the-art architectures like ResNet50 \cite{he2016deep}. Put together, these technologies ensure that VeriForge will find tampered images and continually improve by keeping pace with the changing world of image manipulation.

In a nutshell, VeriForge is a leap into the future of image tampering detection: it merges classical forensic techniques-such as ELA with the latest powers of deep learning for an emerging need to have reliable tools safeguarding digital content in this world where visual communication is well on its way to dominating other forms.

\section{Literature Review}

Image forgery detection has become a very important area of research because of the increasing prevalence of digital image manipulation and its possible misuse in spreading misinformation, undermining trust in media, and enabling cybercrime. Social media platforms have emerged as one of the most used mediums for sharing content. Inappropriate images shared on these platforms can lead to legal consequences. Image manipulation techniques such as copy-move or splicing are standard practices \cite{10747914}. So, there is a need to develop techniques to validate the integrity and authenticity of the images, as these images are considered as evidence in various fields like in investigation or the medical field as medical record or as financial documents, etc. ~\cite{9670931}.

\subsection{Traditional Approaches to Image Forgery Detection}
Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content \cite{zhou2018learning}.
Traditional approaches rely on the inherent changes introduced by manipulation tools. To enhance performance of ELA, data augmentation techniques and meticulous hyperparameter tuning are employed, along with the exploration of alternative model architectures \cite{boustany2024image}.
 Traditional approaches involve resampling features used to capture artifacts, such as JPEG quality loss, upsampling, downsampling, rotation, and shearing \cite{8626149}. 
 
 \subsubsection{Copy-move Forgery} 
 The most frequent form of image fraud is called a copy-move forgery, where a portion of the original image is copied and pasted in a different spot within the same image \cite{10151341}. It simply entails copying picture blocks into the same image and concealing vital information or objects from view \cite{chou2018copy}. A copy move forgery detection method is proposed in this paper using regional Gestalt wavelet analysis structures, which draw upon the rotation invariant ability of uniform local binary patterns and a high-performance texture analysis ability of Gaussian filters.
 
 \subsubsection{Image splicing Forgery} 
 Image splicing is a picture compositing technique that involves joining image fragments from the same or separate images without further post-processing, such as smoothing the borders between the pieces. Image splicing forgery, where parts of one image are pasted and then copied into another image to merge a new image \cite{8456113}. Image splicing fundamentally differs from copy-move in the sense that the pasted region cannot be found elsewhere within the same image \cite{8631389}.
 
 \begin{figure}[h!]
 	\centering
 	\begin{minipage}{0.45\textwidth}
 		\centering
 		\includegraphics[width=\linewidth]{Slicing_Image.png}
 	\end{minipage}
 	\caption{Image Slicing Mechanism, adapted from \cite{8631389}}
 	\label{Slicing Image Mechanism}
 \end{figure}

\subsection{Machine Learning Techniques}
Early machine learning models, such as those using Gaussian Discriminant Analysis (GDA) or Support Vector Machines (SVMs), have been leveraged to classify images based on handcrafted features like Local Binary Patterns (LBP) or texture characteristics. Although promising, these models are often constrained by the limited generalizability of their features to diverse tampering types.

\subsection{CNN Architectures} 
Recent works involving VGG-16, ResNet, and Xception demonstrate that deeper architectures perform better in capturing complex patterns; this is evident from the very best performance of models such as ResNet50. A CNN model trained on ELA images outperforms other pre-trained models \cite{9885600}. CNN uses a pooling layer which reduces the dimension of the data and serves the dual purpose of reducing the enormous amount of output to be fed into the next input layer and by losing some information, the problem of overfitting is solved \cite{9121083}.

\subsubsection{Convolution Layer}
The major function of the convolutional layer is to extract features and to initialize fixed-size kernels to scan the input matrix. The size of the kernels and the stride of sliding are set when a network is to be constructed.
Let \( a_{ij} \) denote the pixel at the \( i \)-th line and the \( j \)-th column of a feature map, the computation of which follows:
\begin{equation*} a_{i,j}=f(\sum_{m=0}^{\mathrm{M}}\sum_{n=0}^{\mathrm{N}}W_{m,n}X_{i+m,j+n}+W_{b}) \tag{1} \end{equation*}

where \( x_{ij} \) denotes the pixel at \((i, j)\) of the input image, \( W_{m,n} \) denotes the shared weight at the corresponding position of filters, \( W_{b} \) is the bias of the filters, and \( f \) denotes the activation function. The size of the kernel is \( M \times N \), meaning that \( m \) ranges from \( 0 \) to \( M \) and \( n \) ranges from \( 0 \) to \( N \) \cite{8456123}.


\subsubsection{ReLU Function}
The method proposed by Selvaraju et. al. \cite{selvaraju2016grad} uses gradient information flowed from the final convolutional layer in our ResNet-50 model to provide, for each pixel in the image, the amount of contribution in final class decision, resulting in an RGB gradient map in which dark red represents areas with high contribution to final decision and dark blue denotes otherwise.

The gradient map G belonging to Ru×v, of width u and height v, given a target class t is

\begin{align*}&\quad \displaystyle \mathbf{G}^{t}=ReLU\overbrace{\left(\sum\limits_{\kappa}\alpha_{\kappa}^{t}A^{\kappa}\right)}^{\text{linear combination}}, \tag{2}\\ & \alpha_{\kappa}^{t}= \overbrace{\hat{\frac{1}{Z}\sum_{i}\sum_{j}}}_{\text{global average pooling}} \frac{\partial y^{t}}{\underbrace{\partial A_{ij}^{\kappa}}_{\text{gradients via backprop}}} \tag{3} \end{align*}

where \( A^{\kappa} \) represents the activation map of the \(\kappa\)-th feature channel of the last convolutional layer. The coefficients \( \alpha_{\kappa}^{t} \) determine the importance of each feature channel for the target class \( t \) and are computed using:

- \( Z \) is the normalization factor, equal to the total number of elements in the activation map (\( Z = u \times v \)),
- \( \frac{\partial y^{t}}{\partial A_{ij}^{\kappa}} \) represents the gradients obtained via backpropagation, showing how the change in the activation \( A_{ij}^{\kappa} \) affects the class score \( y^{t} \),
- The global average pooling (\( \frac{1}{Z} \sum_{i} \sum_{j} \)) aggregates these gradients spatially over the activation map, assigning a single importance weight to each feature channel \( \kappa \).

Only positive contributions of feature maps within ReLU highlight regions supporting the model's prediction for target class \( t \) since ReLU masks negative responses. It combines the information effectively, both in a spatial and feature level, from the activation maps so that the class-relevant region of interest could be well-localized within the input image.

\subsection{Integrating ELA with Deep Learning}
The ELA of Krawtez \cite{krawtez2007pictures} helps to determine the areas in an image at different levels of compression. The method focuses on the lossy compression of the manipulated images to identify them. Original quality itself is a unique feature of the image. The error levels calculated are related to the compression loss. A small change in these error levels implies that the pixel is at its local minima for that particular error rate \cite{ghai2024deep}.
ELA operates as a forensic tool that identifies variations in error levels within compressed images, facilitating the discernment of manipulated regions \cite{10627878}. In case of alteration, traces are left behind in one way or another. In the process of ELA, we compress the image through a lossy function. In our implementation we use the JPEG format for such a purpose.
\begin{equation}
	ELA(n_1, n_2) = \left| X(n_1, n_2) - X_{rc}(n_1, n_2) \right| \tag{4}
\end{equation}

For each colour channel in the above equation, \( n_1 \) and \( n_2 \) represent the row and the column indices, respectively, \( X \) represents the image that is required to be checked for manipulation, and \( X_{rc} \) represents the recompressed image.

The overall error levels are computed by averaging across all colour channels as given below:

\begin{equation}
	ELA(n_1, n_2) = \frac{1}{3} \sum_{i=1}^{3} \left| X(n_1, n_2, i) - X_{rc}(n_1, n_2, i) \right|  \tag{5}
\end{equation}
where \( i = 1, 2, 3 \) for Red, Green, and Blue (RGB) image.

Several distinct artifacts arise from the diverse phases of the picture formation process \cite{10688184}. The detection of photographic splicing by bringing together the high representation power of Illuminant Maps and Convolutional Neural Networks is a way of learning directly from available training data the most important hints of a forgery \cite{8451227}.

Our proposed system is based on these insights; it leverages ELA for feature enhancement and ResNet50 for binary classification. Since the RGB model is one of the most popular color models used in image presentation \cite{10296670}, we use it in parallel with ELA to overcome the limitations of earlier approach. The traditional methods, such as ELA, give valuable insights into the problem, and integrating it with deep architectures such as ResNet50 can provide a strong solution against modern challenges. Our contribution, therefore, to this effort continues with Veriforge: a method that joins strengths from both ELA and deep learning, providing great accuracy and robustness on CASIA V2.


\section{Proposed Method}

In this paper, we demonstrate that combining ELA with CNN model while freezing the base layers and adding custom layers to the base model will provide a significant increase in the robustness of the final model and significantly improve the performance of the classification task to detect images, that are tampered with. We explore this idea in the context of digital forensics, where the goals are to analyze the images and detect suspicious activity in the field of crime, journalism etc. Since digital forensics helps uncover activities and patterns, determine the root causes of incidents, and establish a chain of evidence admissible in court, it becomes essential to identify, recover, analyze and present digital evidence from electronic devices and digital storage. Additionally, an ELA based model helps in identifying the difference in various layers of the images to help us uncover the underlying changes made to the image that is suspected to be tampered with. The model allows us to help various fields where criminal investigations, legal disputes and cybersecurity incidents occur and images need to be analyzed quicky to help user make quick informed decisions. The following sections outline the details of each step in the process.

\subsection{Initialization}

The initialization of the framework is preprocessing the images based on the number of images available in the dataset, it corresponds to the total number of images which are categorized into two classes specifically: 1) Authentic and 2) Tampered. The classes with their labels are split initially to ensure we have a sizeable amount of images to work with, we ensure that the classes are not biased and are equally balanced to provide a robust model and reduce overfitting. The classes are initialized with ones and zeros, zero if the image is authentic, and one if the image has been tampered with. For images to get preprocessed, we need to handle a chunk of images at a time, which will require us to take images in batches to preprocess and analyze further. 

After reshaping the images according to our input shape which will reduce the size of the image so that our next function would work effectively. Creating batches to handle memory overflow and preprocessing images before feeding them to the ELA function ensures that the system doesn’t require a lot of resources and is optimized to create a better and more robust model. 

Further, we take each resized image and input it to our ELA function, this step is iterated over multiple images to get robust images with reduced quality to take forward for our ELA function.

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{Base_Model.png}
	\caption{ Architecture of the base model}
	\label{base-model}
\end{figure*} 

\subsection{ELA Process with Generators}
For each image in our ELA function, a buffer is generated to save the original image but with reduced quality to a temporary in-memory file. This optimizes the memory usage of the system. Compressed images are loaded from the buffer and the difference between the original image and the one which was compressed to get the Pillow image object is calculated. This is a technique used in image forensics to detect tampering or inconsistencies in an image. We enhance the brightness of the image according to the extrema we get in the pillow image object. 

This returns us the ELA image which is a new image that emphasizes regions with differing compression levels, useful for detecting tampering or manipulation. This image will highlight differences due to compression artifacts. To feed the image forward to the model for training, it requires the image to be converted to an array for numerical processing and is normalized making the data compatible with many machine learning models. We close the image further to free up resources and it is a good practice to avoid file locks or memory leaks, especially when processing multiple images in a loop. For the model to handle such a large number of images at once would require a lot of resources, to tackle this issue we provide generators to the model.

Generators are used to preprocess images in batches which are then fed to the model for training. They are advantageous in cases where data augmentation and transformation are a part of the data preprocessing pipeline and the amount of data is significant compared to the computing resources. Instead of loading all images into memory at once, generators load and process batches on-the-fly, reducing memory consumption. Generators often include on-the-fly preprocessing, which enhances the dataset and helps the model generalize better, increasing its robustness and generalizability for better performance.

We have also visualized a batch of images after the ELA function to ensure that the shape of the generator and preprocessed images are according to the requirements.

\subsection{Resnet50 Model Architecture with Training Evolution }
In this study, we employ a Resnet50 model to classify the images into two categories (Authentic and Tampered), we use a transfer learning approach based on the ResNet50 architecture, pre-trained on ImageNet to solve our classification problem. This method capitalizes on the robust feature extraction capabilities of ResNet50, significantly reducing training time and computational resources while improving model accuracy. The model was customized to adapt the pre-trained base for the task by introducing additional trainable (Custom) layers specifically designed for binary classification. Figure \ref{base-model}: shows the architecture of the base model we have used featuring Resnet50 in a five-stage design to represent the overall function of the layers.

The base of our architecture is the ResNet50 model, known for its depth and efficient handling of the vanishing gradient problem through residual connections. The model was loaded without the top layers, removing the full-connected top layers helped us retain only the convolutional layers which are responsible for extracting hierarchical features from the input images. The input shape was fixed to match the requirement of the model while ensuring compatibility with pre-trained weights. The freezing of all layers in the base model during training aimed at preserving the learned weights and keeping the feature extraction capability unaltered by the new task-specific data. To this binary classification problem, we attached five different types of custom layers to the base pre-trained model in an attempt to make it adapt. First, there is a global average pooling layer that reduces the high dimensional feature maps output by ResNet50 into a single vector for each feature map, reducing the number of trainable parameters and hence improving generalization. Unlike the traditional Flatten layer, this preserves spatial information and improves model performance when using ResNet based architectures. Further, we added batch normalization to normalize the activations from the pooling layer, reducing internal covariate shift and accelerating convergence. 

Once we are done with that, we added a series of two dense layers. The first layer consists of 512 neurons providing substantial capacity to capture complex patterns. The second contains 256 neurons for further refinement of feature extraction. Both are activated by the ReLU activation function, which introduces non-linearity in deep networks and provides output zero for negative inputs, creating sparsity in the activations that could further be helpful in efficient computations with a reduction in overfitting while being computationally less expensive than the sigmoid or tanh function. The custom layers then involve a dropout regularization technique which was employed after each dense layer with a rate of 0.75 after the first dense layer to impose strong regularization and a rate of 0.5 was applied after the second dense layer for moderate regularization. This helped in preventing overfitting of the model during training. After dropout regularization, there was a single neuron in the output layer with sigmoid activation to predict the probability of the positive class, which is suitable for binary classification tasks.

It achieves a very good trade-off among accuracy, training efficiency, and robustness, making it pretty effective in real-world applications. This model was trained for more than 100 epochs with a step size of about 145 while capturing essential evaluation metrics such as accuracy, the f1 score, and validation accuracy. This will make sure we minimize the gap between the training and validation accuracy of the model, which will be robust and very effective in real-world applications.

\section{Results and Analysis}

\subsection{Datasets}
	The CASIA V2 dataset, created by the Institute of Automation at the Chinese Academy of Sciences, CASIA, is one of the more general resources for research in the area of image forgery detection. Designed to support studies on image tampering, it includes manipulative techniques such as splicing, copy-move forgery, and other methods. The Table \ref{casiav2overview} outlines the details of the dataset and gives us an overview of what the dataset contains. The dataset is a very diverse collection of 12,614 images of roughly equal amounts of authentic and tampered examples from various categories such as nature, animals, objects, and urban settings. Such diversity will enable models trained with CASIA V2 to generalize rather well across different scenarios. There are also ground truth masks provided for every tampered image in the dataset, highlighting manipulated regions and thus making the dataset indispensable for supervised learning tasks and performance evaluation.
	\begin{table}[h!]
		\centering
		\setlength{\tabcolsep}{3pt}
		\vspace{3mm}
		\begin{tabular}{|p{75pt}|p{150pt}|p{115pt}|}
			\hline
			\textbf{Feature}            & \textbf{Description}                                                                 \\ \hline
			\textbf{Dataset Name}       & CASIA TIDE v2.0                                                                      \\ \hline
			\textbf{Purpose}            & Designed for research and development in image forgery detection                     \\ \hline
			\textbf{Released By}        & Chinese Academy of Sciences Institute of Automation (CASIA)                          \\ \hline
			\textbf{Image Type}         & Digital images, both authentic and tampered                                          \\ \hline
			\textbf{Number of Images}   & 12,614 images (7,491 authentic and 5,123 tampered)                                   \\ \hline
			\textbf{Forgery Techniques} & Splicing, copy-move, and other common image tampering techniques                      \\ \hline
			\textbf{Resolution}         & Varies (typically medium resolution)                                                 \\ \hline
			\textbf{File Format}        & JPEG (for compressed images)                                                         \\ \hline
			\textbf{Dataset Structure}  & Contains two folders: one for authentic images and one for tampered images           \\ \hline
			\textbf{Applications}       & Used for image forgery detection, digital forensics, and tampered image localization research \\ \hline
			\textbf{Public Availability} & Available for academic and non-commercial use upon request from CASIA               \\ \hline
		\end{tabular}
		\caption{Overview of the CASIA TIDE v2.0 Dataset}
		\label{casiav2overview}
	\end{table}
	
	
	The resolution of the images also varies to reflect real-world conditions, as images naturally come from different devices and sources. CASIA V2 has a variety of applications, including training and validation of image forgery detection models, testing of pre-processing techniques such as Error Level Analysis, and benchmarking deep learning architectures like CNNs for forgery classification. Features such as these make it invaluable in the field of digital forensics and research on image authenticity.
	
	
\subsection{Results Evaluation}
	\begin{table}[h!]
		\centering
		\scriptsize % Shrinks the font size
		\setlength{\tabcolsep}{4pt} % Reduces column spacing
		\renewcommand{\arraystretch}{1.2} % Adjusts row height for better readability
		\begin{tabular}{@{}lccc@{}}
			\toprule
			\textbf{Reference}            & \textbf{Model}                       & \textbf{Accuracy} \\ \midrule
			Jabaar et al.  (2021)      & ELA, CNN                             & 93.8\%                          \\
			Mashaan \& Ahmed  (2023)   & Local Binary Patterns, Tamura        & 96\%                             \\
			Pandiyan  (2023)           & Xception                             & 95\%                               \\
			Singh et al.  (2023)       & ELA, CNN                             & 98\%                              \\
			Geethanjali et al. (2024) & ELA, CNN                             & 95\%                              \\
			Sarkar et al. (2024)      & SegNet, MobileNet                    & 89\%                            \\
			Anvekar et al.  (2024)     & CNN                                  & 96\%                               \\
			Roy et al. (2024)         & Modified MultiResUnet Architecture   & 98\%                               \\ \midrule
			\textbf{Proposed Method}      & VGG16 + ELA                          & 83\%                          \\
			\textbf{Proposed Method}      & ResNet50 + Improved ELA              & 89\%                          \\ \bottomrule
		\end{tabular}
		\caption{Summary of Different Studies and Quantitative Results}
		\label{tab:summary-studies}
	\end{table}
   This section compiles the quantitative results and presents an analysis of the project outcomes. The performance of ResNet50 and VGG16 CNN models trained on CASIA V2 dataset by using traditional methods like ELA will be judged on Accuracy and F1 metrics. Accuracy measures the overall correctness of the model's predictions and at the same time, the F1 score represents the balance between precision and recall, which is suitable for imbalanced datasets. In other words, it shows the likelihood that the model correctly classifies an image with its true label.

  According to the qualitative results, RestNet50, combined with improved ELA, performed better than VGG16, which used traditional ELA.
  
\subsection{Analysis of the Initial VGG16 Model}
    \begin{figure}[h!]
  	\centering
  	\begin{minipage}{0.45\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{LOSS_VGG16.png}
  	\end{minipage}
  	\hfill
  	\begin{minipage}{0.45\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{VGG16_ACC.png}
  	\end{minipage}
  	\caption{VGG16 Model: Loss and Accuracy Graphs}
  	\label{fig:vgg16-graphs}
  \end{figure}
  
  The first prototyping at the earlier stages of the project used a VGG16 CNN model combined with traditional ELA preprocessing. This approach focused on finding edges to detect forged regions on a grey-scale-converted image. Although this yielded a promising 83\% in validation accuracy, the training and validation graphs were too inconsistent and abrupt, making the F1 score stand at a low of 0.86. That meant the balance between precision and recall had to be promoted, reducing the higher number of false positives and false negatives, which could be achieved by using more sophisticated preprocessing techniques. The model VGG16 has anomalies shown in Figure due to sudden changes and also huge differences between Training and Validation accuracy graphs. Because the model failed to generalise into a valid test set, it could be defined that the model went to overfit while in training, also justified with the low F1 value of the VGG16 model.
  
  Figure~\ref{fig:vgg16-graphs} reveals inconsistencies, with abrupt variations and a wide gap between the Training and Validation accuracy graphs for the VGG16 model. As the model failed to generalize on the validation test set, it can be termed that the model went overfitting during the training, which is evident by the low F1 score of the VGG16 model.
  
\subsection{ResNet50 Model and Improved ELA with Evaluation Metrics}
    \begin{figure}[h!]
  	\centering
  	\begin{minipage}{0.23\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{Authentic_img.png}
  	\end{minipage}
  	\hfill
  	\begin{minipage}{0.21\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{Forgery_img.png}
  		
  	\end{minipage}
  	\caption{Authentic and Forgery Predicted Label images}
  	\label{fig:Authentic_and_ELA}
  \end{figure}
  
  Resnet50 with an enhanced ELA preprocessing was implemented to address these limitations. The Improved preprocessing technique modified the ELA to analyze the RGB channels instead of grayscale. This allowed the model to detect colour discrepancies and variations in JPEG compression artifacts which are more effective in detecting forgery compared to finding edges.
  The improved ELA led to the clear identification of tampered regions, as shown in Fig~\ref{fig:Authentic_and_ELA}.
  
  \begin{figure}[h!]
  	\centering
  	\begin{minipage}{0.45\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{Graph_LOSS.jpg}
  	\end{minipage}
  	\hfill
  	\begin{minipage}{0.45\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{Graph_ACC.jpg}
  		
  	\end{minipage}
  	\caption{Resnet50 Model: Loss and Accuracy Graphs}
  	\label{fig:Resnet50 Loss and Accuracy Graphs}
  \end{figure}
  Besides that, compared to VGG16, several advantages have been gained with the ResNet50 architecture. Instead of flattening the feature maps, global average pooling was used that reduces overfitting because of minimized trainable parameters. More than that, batch normalisation layers stabilize training by normalizing the inputs, and dropout with various intensities added to the regularization helped reduce overfitting. With these changes, training-validation accuracy curves were smoother and more convergent, as depicted in Fig ~\ref{fig:Resnet50 Loss and Accuracy Graphs}.
 
 
 \begin{figure}[h!]
 	\centering
 	\begin{minipage}{0.32\textwidth}
 		\centering
 		\includegraphics[width=\linewidth]{Confusion_Matrix.jpg}
 	\end{minipage}
 	\hfill
 	\begin{minipage}{0.32\textwidth}
 		\centering
 		\includegraphics[width=\linewidth]{Chart_EM.jpg}
 		
 	\end{minipage}
 	\caption{Confusion Matrix and Evaluation Chart}
 	\label{fig:Confusion Matrix}
 \end{figure}
 The improved ELA coupled with transitioning from VGG16 to Resnet50 significantly reduced the number of false positives. Smoothening of both the validation and training accuracy curves showed reduced overfitting compared to the curves of the VGG16 model. The resultant 89\% accuracy and 0.92 weighted F1 score reflect the better generalization capability of the model compared to VGG16. A higher F1 score indicated a balanced recall and precision where false positives and false negatives are minimized as evident in Fig ~\ref{fig:Confusion Matrix}. 
 
 It is noteworthy to mention that data augmentation techniques – rotation, flip, zoom, width shift, height shift, etc. – when applied did not have any effect on forged images. The best possible explanation for this phenomenon is that data augmentation does not present any variation in the forged region. After finalizing the model configuration, training, and validation, it was programmatically uploaded to Hugging Face, from where it would be remotely instantiated in the backend service layer using the API provided by Hugging Face. Python Django was used to create a backend application to serve user requests, process the images present in the user requests, predict the labels, and respond with the predicted results.
  
\section{Conclusion}
This study is essential in maintaining integrity in crucial fields like Law enforcement, Cyber security, and Legal investigations. In this project, we created a reliable system that can identify image manipulation. The study contrasts the effectiveness of two well-known CNN models, VGG16 and ResNet50, with traditional preprocessing methods like ELA and data augmentation. The project concludes with viable results that taking colour inconsistencies and compression differences significantly improves model capability compared to conventional grayscale-based ELA methods. The system can be integrated into social media to moderate content in real-time, flagging misleading images and advancing platform integrity. This needs to be handled in future work by enrichment of the dataset with diversity and updates for further strengthening the resistance and longevity of the model. Moreover, interpreting it with Explainable AI by using Grad-CAM will illustrate the regions of interest influential in the model's choices for transparent decisions and gaining the trust of the users.
The dataset and code for further research are available \href{https://www.kaggle.com/datasets/divg07/casia-20-image-tampering-detection-dataset/data}{[here]} and \href{https://github.com/ayaz168/veriforge}{[repository link]}.

\section*{Acknowledgment}
We would like to express our sincere gratitude to Prof. ANK Zaman for his invaluable guidance and support throughout the course of this research. His insightful suggestions, expert advice, and continuous encouragement significantly contributed to the successful completion of this project. We also appreciate his assistance in refining our methodology and providing constructive feedback, which greatly enhanced the quality of our work. This research would not have been possible without his mentorship.

\balance
\bibliographystyle{IEEEtran}
\bibliography{sn-bibliography.bib}

%\begin{thebibliography}{00}
%
%\bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.
%
%\bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.
%
%\bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402.
%
%\bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965.
%
%\bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published.
%
%\bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.
%
%\bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.
%
%\bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.
%
%\bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.
%
%\bibitem{b10} G. O. Young, ``Synthetic structure of industrial
%plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters,
%Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64.
%[Online]. Available:
%\underline{http://www.bookref.com}.
%
%\bibitem{b11} \emph{The Founders' Constitution}, Philip B. Kurland
%and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987.
%[Online]. Available: \underline{http://press-pubs.uchicago.edu/founders/}
%
%\bibitem{b12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014.
%[Online]. Available:
%\underline{http://dl.z-thz.com/eBook/zomegaebookpdf\_1206\_sr.pdf}. Accessed on: May 19, 2014.
%
%\bibitem{b13} Philip B. Kurland and Ralph Lerner, eds., \emph{The
%Founders' Constitution.} Chicago, IL, USA: Univ. of Chicago Press,
%1987, Accessed on: Feb. 28, 2010, [Online] Available:
%\underline{http://press-pubs.uchicago.edu/founders/}
%
%\bibitem{b14} J. S. Turner, ``New directions in communications,'' \emph{IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995.
%
%\bibitem{b15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' \emph{Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986.
%
%\bibitem{b16} P. Kopyt \emph{et al., ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' \emph{IEEE THz Sci. Technol.,} to be published. DOI: 10.1109/TTHZ.2016.2544142.
%
%\bibitem{b17} PROCESS Corporation, Boston, MA, USA. Intranets:
%Internet technologies deployed behind the firewall for corporate
%productivity. Presented at INET96 Annual Meeting. [Online].
%Available: \underline{http://home.process.com/Intranets/wp2.htp}
%
%\bibitem{b18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: \underline {http://CRAN.R-project.org/package=raster}
%
%\bibitem{b19} Teralyzer. Lytera UG, Kirchhain, Germany [Online].
%Available:
%\underline{http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home}, Accessed on: Jun. 5, 2014.
%
%\bibitem{b20} U.S. House. 102\textsuperscript{nd} Congress, 1\textsuperscript{st} Session. (1991, Jan. 11). \emph{H. Con. Res. 1, Sense of the Congress on Approval of}  \emph{Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS
%
%\bibitem{b21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES
%
%\bibitem{b22} D. B. Payne and J. R. Stern, ``Wavelength-switched pas- sively coupled single-mode optical network,'' in \emph{Proc. IOOC-ECOC,} Boston, MA, USA, 1985, pp. 585--590.
%
%\bibitem{b23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the \emph{2\textsuperscript{nd} Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984.
%
%\bibitem{b24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978.
%
%\bibitem{b25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.
%
%\bibitem{b26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
%
%\bibitem{b27} A. Harrison, private communication, May 1995.
%
%\bibitem{b28} B. Smith, ``An approach to graphs of linear forms,'' unpublished.
%
%\bibitem{b29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.
%
%\bibitem{b30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.
%
%\bibitem{b31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.
%
%\bibitem{b32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' \emph{Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103.~
%
%\bibitem{b33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' \emph{IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111
%
%\bibitem{b34} S. Azodolmolky~\emph{et al.}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~\emph{J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.
%
%\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Rishi_Formal_Pic.jpg}}]{Rishikesh Ravi} is pursuing a Master of Applied Computing at Wilfrid Laurier University, Waterloo, ON, expected to graduate in December 2024. He earned his Master of Computer Applications from Veermata Jijabai Technological Institute, Mumbai, India in 2021.
	
	From July 2021 to June 2023, he served as a Jr. Software Developer at Locobuzz Solutions in Mumbai, where he enhanced mobile applications for Android and cross-platform environments using Xamarin Forms. His contributions included optimizing performance and revamping user interfaces, which significantly improved user engagement. He also interned at Thomas Cook India Ltd., developing web applications and REST APIs. He is dedicated to creating innovative software solutions that enhance user experiences.
\end{IEEEbiography}



\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Ayaz_Formal_Pic.jpg}}]{Ayaz Afzal} is currently pursuing a Master of Applied Computing at Wilfrid Laurier University in Waterloo, ON, Canada. He previously received a Bachelors of Science (Computer Science) from National University of Computer and Emerging Sciences, Islamabad, Pakistan  in 2021.
	
	From 2022 to 2023, he worked as a Software Developer,  utilizing modern programming frameworks to deliver high quality scalable SaaS products in the area of Blockchain. He is proficient in Python, Java, Javascript and modern software practices. 
	
	During his undergraduate he has worked on several projects in the area of Computer Vision. His final year project was in the area of collision detection and avoidance for unmanned aerial vehicles.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Hamza_Formal_Pic.png}}]{Hamza Rangwala} is pursuing a Master of Applied Computing at Wilfrid Laurier University, Waterloo, ON, expected to graduate in December 2024. He has completed significant coursework in artificial intelligence, data analytics, and machine learning.
	
	From September 2023 to January 2024, he worked as a Research Assistant at Wilfrid Laurier University, contributing to projects focused on natural language processing and predictive analytics. Previously, as a Software Engineer at Solution-Wise, he developed scalable software solutions and implemented machine learning models for user discovery and geolocation tagging. 
	
	Proficient in Java, Python, JavaScript, and Dart, Mr. Hamza A. Rangwala has developed expertise in using frameworks and tools such as TensorFlow, Flutter, and AWS. His recent projects include Web application utilizing OpenAI's API for text analysis. He is passionate about leveraging AI and ML technologies to solve complex real-world problems.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Trushit_Formal_Pic.jpg}}]{Trushit Patel} received the Bachelors of Technology degree in Computer Science and Engineering from the India University, Ahmedabad, in 2023 and is currently pursuing a Master's degree in Applied Computing at Wilfrid Laurier University, which he is expected to complete in December, 2024. 
	His professional experience includes roles as an AI Trainer and Software Development Engineer, focusing on AI data annotation, real-time applications, and scalable software solutions. Trushit's research and professional interests lie in artificial intelligence, software development, and microservices architecture.
	
\end{IEEEbiography}

\EOD

\end{document}
